# 웹 로봇

- 웹 로봇은 사람과의 상호작용 없이 연속된 웹 트랜잭션들을 자동으로 수행하는 소프트웨어 프로그램으로 ‘크롤러’, ‘스파이더’, ‘웜’, ‘봇’ 등 각양각 색의 이름불립니다.

## 9.1 크롤러와 크롤링

- 웹 크롤러는, 먼저 웹페이지를 한 개 가져오고, 그 다음 그 페이지가 가리키는 모든 웹페이지를 가져오고, 다시 그 페이지들이 가리키는 모든 웹페이지들을 가져오는, 이러한 일을 재귀적으로 반복하는 방식으로 웹을 순회하는 로봇입니다.
- HTML 하이퍼링크 들로 만들어진 웹을 따라 기어다니기(crawl) 하기 때문에 크롤러 혹은 스파이더라고 부릅니다.

### 9.1.1 어디에서 시작하는가: ‘루트 집합’

- 크롤러가 방문을 시작하는 URL들의 초기 집합은 루트 집합(root set)이라고 부릅니다.
- 일반적으로 좋은 루트 집합은 크고 인기 있는 웹 사이트(http://www. yahoo.com 등)입니다.

### 9.1.2 링크 추출과 상대 링크 정상화

- 크롤러들은 간단한 HTML 파싱을 해서 이들 링 크들을 추출하고 상대 링크를 절대 링크로 변환할 필요가 있습니다.(2.상대 URL에 다룰 예정)

### 9.1.3 순환 피하기

- 루프나 순환에 빠지지 않도록 매우 조심해야합니다.

### 9.1.4 루프와 중복

- 같은 페이지들을 반복해서 가져오는데 모든 시간을 허비하고 이러한 크롤러가 네트워크 대역폭을 다 차지하고 그 어떤 페이지도 가져올 수 없게 되어버릴 수 있습니다.
- 크롤러가 같은 페이지를 반복해서 가져오면 고스란히 웹 서버의 부담이 됩니다.
- 비록 루프 자체가 문제가 되지 않더라도, 크롤러는 많은 수의 중복된 페이지들을 가져오게 되고 쓸모없게 만드는 중복된 콘텐츠로 넘쳐나게 됩니다.

### 9.1.5 빵 부스러기의 흔적

- 대규모 웹 크롤러가 그들이 방문한 곳을 관리하기 위해 사용하는 유용한 기법
  - 트리와 해시 테이블 : URL을 추적하기 위해 검색 트리나 해시 테이블을 사용하면 URL을 훨씬 빨리 찾아볼 수 있습니다.
  - 느슨한 존재 비트맵 : 공간 사용을 최소화하기 위해 비트 배열과 같은 느슨한 자료 구조를 사용합니다.
  - 체크포인트 : 중단될 경우를 대비해서 방문한 URL의 목록이 디스크 에 저장합니다.
  - 파티셔닝 : 몇몇 대규모 웹 로봇은, 각각이 분리된 한 대의 컴퓨터인 로봇들이 동시에 일하고 있는 ‘농장(farm)’을 이용합니다.

### 9.1.6 별칭(alias)과 로봇 순환

- 다른 URL들이 같은 리소스를 가리키게 되는 몇 가지 이유가 있습니다.
  - 기본 포트가 80번
  - %7F0| ~과 같을 때(이스케이프 문자열)
  - 태그에 따라 페이지가 바뀌 지 않을 때
  - 서버가 대소문자를 구분하지 않을 때
  - 기본 페이지가 index.html 일때
  - 도메인과 같은 아이피 주소

### 9.1.7 URL 정규화하기

- 웹 로봇은 URL를 정규화 시킵니다.
  - 80번 포트 추가
  - 이스케이프 문자열 치환
  - 태그 제거

### 9.1.8 파일 시스템 링크 순환

- 파일 시스템의 심벌릭 링크는 사실상 아무것도 존재하지 않으면서도 끝없이 깊어지는 디렉터리 계층을 만들 수 있기 때문에, 매우 교묘한 종류의 순환을 유발할 수 있습니다.

![symbolic_link.png](./image/symbolic_link.png)

### 9.1.9 동적 가상 웹 공간

![dynamic_web_disk.png](./image/dynamic_web_disk.png)

### 9.1.10 루프와 중복 피하기

- URL 정규화
- 너비 우선 크롤링

  방문할 URL들을 웹 사이트들 전체 에 걸쳐 너비 우선으로 스케줄링하면, 순환의 영향을 최 소화할 수 있습니다.

- 스로틀링

  일정 시간 동안 가져올 수 있는 페이지의 숫자를 제한합니다.

- URL 크기 제한

  로봇은 일정 길이(보통 1KB)를 넘는 URL의 크롤링은 거부할 수 있습니다.

- URL/사이트 블랙리스트

  문제를 일으키는 사이트나 URL이 발견될 때마다 이 블랙리스트에 추가합니다.

- 패턴 발견

  반복되는 구성요소를 가진 URL을 잠재적인 순환으로 보고, 둘 혹은 셋 이상의 반복된 구성요소를 갖고 있는 URL을 크롤링하는 것을 거절합니다.

- 콘텐츠 지문(fingerprint)

  콘텐츠 지문을 사용하는 로봇들은 페이지의 콘텐츠에서 몇 바이트 를 얻어내어 체크섬(checksum)을 계산하고 만약 로봇이 이전에 보았던 체크섬을 가진 페이지를 가져온다면, 그 페 이지의 링크는 크롤링하지 않습니다.

- 사람의 모니터링

  진단과 로깅을 포함하도록 설계하여 사람이 인지할 수 있게끔 합니다.

## 9.2 로봇의 HTTP
