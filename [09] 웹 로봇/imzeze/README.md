# 웹 로봇

웹 로봇은 사람과의 상호작용 없이 연속된 웹 트랜잭션들을 자동으로 수행하는 소프트웨어 프로그램이다. 많은 로봇들이 웹사이트를 돌아다니면서 콘텐츠를 수집하고, 하이퍼링크를 따라 이동하며 발견한 데이터를 처리한다.

> 예시
>
> - 주식 시장 서버에 매 분마다 데이터 조회 요청을 보내고, 받은 데이터로 주식 추이 그래프를 그리는 주식 그래프 로봇
> - 검색 데이터베이스를 만들기 위해 발견한 모든 문서를 수집하는 검색 엔진 로봇
> - 상품에 대한 가격 데이터베이스를 만들기 위해 온라인 쇼핑몰에서 웹페이지를 수집하는 가격 비교 로봇

<br />

## 크롤러와 크롤링

웹 크롤러는 가져온 웹페이지가 가리키는 모든 웹페이지를 가져오고, 다시 가져온 웹페이지가 가리키는 모든 웹페이지를 가져오는, 재귀적으로 반복해서 웹을 순회하는 로봇이다.  
크롤러가 가져온 문서들은 검색할 수 있는 데이터베이스로 만들어진다.

### 어디에서 시작하는가: ‘루트 집합’

크롤러가 초기에 접속하는 URL을 `루트 집합(root set)`이라고 한다. 웹에서는 모든 문서로 이어지는 하나의 문서란 없다. 다른 웹페이지와 어떤 연결고리도 없는 웹페이지가 있을 수 있기 때문이다. 따라서 좋은 루트 집합이란 아래와 같다.

- 크고 인기 있는 웹사이트
- 새로 생성된 페이지
- 잘 알려져 있지 않은 페이지

### 링크 추출과 상대 링크 정상화

크롤러는 검색한 페이지에 있는 URL 링크들을 파싱해 크롤링할 페이지 목록에 추가한다. 크롤링을 함에 따라 이 목록은 급격히 증가한다.

### 순환 피하기

`A -> B -> C -> A`와 같이 크롤링 하면서 얻은 페이지를 방문하다가 순환에 빠지지 않도록 그들이 어디를 방문했는지 알아야 한다. 순환은 다음의 이유로 크롤러에 해롭다.

- 크롤러를 루프에 빠뜨려서 꼼짝 못하게 만들 수 있다. 같은 페이지들을 반복해서 가져오는데 시간이 낭비되고, 네트워크 대역폭을 차지해 다른 페이지를 가져오는 것 조차 막을 수 있다.
- 웹 서버에 부하를 가져와 실제 사용자가 웹 페이지에 접근하는 것을 막을 수 있다.
- 중복된 콘텐츠만 제공하는 쓸데없는 애플리케이션으로 전락할 수 있다.

### 빵 부스러기의 흔적

어떤 URL을 방문하였는지 지속적으로 추적하는 일은 쉽지 않다.
URL이 굉장히 많기 때문에 어떤 URL에 접속했는지 판단하기 위해 속도와 메모리 사용 면에서 효율적인 자료구조가 사용되어야한다.

- 검색 트리와 해시 테이블  
   방문한 URL을 빠르게 찾을 수 있는 자료구조이다.
- 느슨한 존재 비트맵  
   공간 사용을 최소화하기 위해 `존재 비트 배열`과 같은 느슨한 자료 구조를 이용한다. 각 URL은 해시함수에 의해 고정된 크기의 숫자로 변환되고 배열안에 대응하는 **존재 비트**를 갖는다. 이후 어떤 URL이 크롤링이 되었을 때, 존재 비트가 이미 있다면 크롤링한 적이 있는 URL로 간주한다.
- 체크포인트  
   크롤러가 갑작스럽게 중단될 경우에 대비해 방문한 URL 목록이 디스크에 저장되었는지 확인한다.
- 파티셔닝  
   웹이 성장하면서 한 대의 컴퓨터에서 하나의 로봇만을 이용해 크롤링하기에는 메모리, 연산 능력 등이 충분하지 못해 여러 대의 컴퓨터에서 여러 로봇이 URL을 분담해 크롤링하기 시작했다.

### 별칭과 로봇 순환

올바른 자료 구조를 사용하더라도 URL이 별칭을 갖는 경우 해당 URL이 크롤링된 적 있는 URL인지 파악하기 쉽지 않다.

> 예시  
> http://www.foo.com/ || http://www.foo.com/index.html  
> http://www.foo.eom/x.html#early || http://www.foo.eom/x.html#middle

### URL 정규화하기

웹 로봇은 URL을 표준 형식으로 정규화하여 다른 URL과 같은 리소스를 판단하고 있는지 판단한다.

- 포트번호가 없다면 80으로 간주한다.
- 이스케이핑된 문자들은 대응되는 문자로 변환한다.
- `#` 태그들을 제거한다.

그러나 이외에 웹 로봇이 웹서버에 대한 지식없이는 중복을 피할 수 없는 경우도 있다.

- url에 대소문자 차이가 있는 경우 웹서버의 구분 여부
- 웹 서버의 색인 페이지 설정에 대한 정보
- url에 호스트명이 있는 경우와 IP 주소가 있는 경우에 두 주소의 동일 여부

### 파일 시스템 링크 순환

> 심볼링 링크 ≒ 바로가기 링크
> <img width="634" alt="image" src="https://github.com/flataex/http-the-definitive-guide-study/assets/67260437/0caf45ab-02d7-41c8-bc8a-be99b6ab3e69">

- a의 파일시스템에서 크롤러의 수행동작은 다음과 같다.
  1. www.foo.com/index.html에서 subdir/index.html로 이어지는 링크를 발견한다.
  2. www.foo.com/subdir/index.html에서 subdir/1ogo.gif로 이어지는 링크를 발견한다.
  3. www.foo.com/subdir/logo.gif를 가져오고 더이상 링크가 없어 끝이 난다.
- b의 파일시스템에서 크롤러의 수행동작은 다음과 같다.

  1. www.foo.com/index.html에서 subdir/index.html로 이어지는 링크를 발견한다.
  2. www.foo.com/subdir/index.html을 가져왔지만 같은 index.html로 돌아간다.
  3. www.foo.com/subdir/subdir/index.html을 가져온다.
  4. 다시 되돌아가 www.foo.com/subdir/subdir/subdir/index.html을 가져온다.

  `subdir/` 이 `/`를 가리키는 심볼릭 링크로, 크롤러는 이를 다른 url로 인식해 순환에 빠지게 된다.

### 동적 가상 웹 공간

악의적인 웹서버가 동적으로 가상 URL을 가진 HTML을 무한히 생성해 크롤러가 정에 빠지는 경우가 있다. 동적 콘텐츠로 인해 발생할 수 있는 이러한 결과 때문에, 많은 로봇이 URL의 어딘가에 ‘cgi’라는 문자열을 포함한 사이트 크롤링을 거부한다.

### 루프와 중복 피하기

모든 순환을 피하는 완벽한 방법은 없다. 실제로 잘 설계된 로봇은 순환을 피하기 위해 휴리스틱 집합을 필요로 한다. 휴리스틱은 문제를 피하는데 도움을 주지만 동시에 약간 손실을 유발할 수도 있다. 왜냐하면 의심스러워 보이지만 실은 유효한 콘텐츠를 걸러버리게 되는 일도 일어날 수 있기 때문이다.

이러한 웹에서 로봇이 더 올바르게 동작하기 위해 사용하는 기법들은 다음과 같다.

- URL 정규화
- 너비 우선 크롤링  
   깊이 우선 크롤링 방식을 운용하다 함정에 빠지면 다른 사이트로 빠져나가기 어렵기 때문에 너비 우선 크롤링 방식으로 운용한다.
- 스로틀링  
   일정 시간동안 가져올 수 있는 웹 사이트 개수를 제한한다.  
   만약 로봇이 순환에 빠져 해당 사이트에 지속적으로 접근한다면 스로틀링을 통해 해당 서버에 대한 접근 횟수와 중복 횟수를 제한할 수 있다.
- URL 크기 제한  
   로봇은 일정 크기를 넘어가는 URL에 대한 크롤링을 거부할 수 있다. 순환으로 인해 URL의 크기가 계속 커지다보면 크롤러에 의해 거부될 수 있을 것이다. 이 기법을 적용하면 가져오지 못하는 콘텐츠들도 틀림없이 있을 것이라는 단점이 있다.
- URL/사이트 블랙리스트  
   순환에 빠지거나 함정인 것으로 알려진 URL을 블랙리스트에 추가한다. 블랙리스트는 크롤링 되는 것을 싫어하는 특정 사이트를 피하기 위해 사용될 수 있다.
- 패턴 발견  
   로봇은 심볼링 링크로 인한 순환과 같이, 반복되는 구성요소를 가진 URL을 잠재적인 순환으로 보고, 크롤링하는 것을 거절한다.
- 콘텐츠 지문  
   지문은 중복을 감지하는 직접적인 방법이다. 로봇들은 페이지의 콘텐츠에서 몇 바이트를 얻어내어 `체크섬(checksum)`을 계산한다. 전에 보았던 체크섬을 가진 페이지를 가져온다면, 그 페이지의 링크는 크롤링하지 않는다. 지문 생성용으로는 MD5와 같은 메시지 요약 함수가 인기 있다.
- 사람의 모니터링  
   로봇의 진행 상황을 모니터링해서 뭔가 특이한 일이 일어나면 즉각 인지할 수 있게끔 반드시 진단과 로깅을 포함하도록 설계되어야 한다.

<br />

## 로봇의 HTTP

로봇 또한 HTTP 명세의 규칙을 지켜야 한다. 로봇은 HTTP 요청을 만들고 적절한 HTTP 요청 헤더를 사용해야 한다.

### 요청 헤더 식별하기

로봇은 신원 식별 헤더를 구현하고 전송한다.  
| 헤더 | 설명 |
|--|--|
|User-Agent|서버에게 요청을 만든 로봇의 이름을 말해준다.|
|From|로봇의 사용자/관리자의 이메일 주소를 제공한다. |
|Accept|서버에게 어떤 미디어 타입을 보내도 되는지 말해준다.|
|Referer|현재의 요청하는 URL이 포함된 문서의 URL을 제공한다. 어떻게 로봇이 그들의 사이트를 발견했는지 알아내고 싶은 사이트 관리자들에게 매우 유용하다. |

### 가상 호스팅

로봇 구현자들은 Host 헤더를 지원할 필요가 있다. 요청에 Host 헤더를 포함하지 않으면 로봇이 어떤 URL에 대해 잘못된 콘텐츠를 찾게 만든다.

> 5장 참고  
> 서버에 일반적으로 웹 컨텐츠를 저장하는 폴더를 만들어 두는데, 이 폴더를 `docroot` 라고 부른다. 서버는 HOST별로 docroot를 다르게 설정할 수 잇다.
>
> docroot ) DocumentRoot /usr/local/httpd/files  
> 요청 URI ) /specials/saw-blade.gif

### 조건부 요청

로봇이 검색하는 양을 최소화하기 위해 시간이나 엔터티 태그를 비교할 수 있는 조건부 헤더를 보내기도 한다.

### 응답 다루기

로봇이 서버와 상호작용을 하고자 한다면 HTTP 상태코드나 HTTP 헤더에 임베딩된 정보를 토대로 엔터티 정보를 이해할 필요가 있다.

### User-Agent 타겟팅

웹 관리자들은 로봇들로부터의 요청을 예상해야 한다.  
많은 웹 사이트들은 그들의 여러 기능을 지원할 수 있도록 브라우저의 종류를 감지하여 그에 맞게 콘텐츠를 최적화한다. 만약 지원하지 않는 브라우저라면 로봇에 에러 메세지를 반환한다.

<br />

## 부적절하게 동작하는 로봇들

- 폭주하는 로봇  
  로봇은 빠르게 HTTP 요청을 생성한다. 때문에 논리적인 에러를 갖고 있거나 순환에 빠졌다면 웹 서버에 극심한 부하를 안겨줄 수 있어, 다른 클라이언트가 서비스를 받지 못하는 일이 생길 수 있다.

- 오래된 URL  
  웹 사이트의 콘텐츠가 수정된 경우에도 로봇은 존재하지 않는 문서에 접근 요청을 보낼 수 있다. 이 에러가 에러 로그에 쌓이는 문제점이나 에러 페이지를 제공하는 데에 부하가 생기는 문제점이 서버에 생길 수 있다.

- 길고 잘못된 URL  
  순환이나 프로그래밍상의 오류로 인해 로봇은 길이가 긴 URL을 요청할 수 있다. 이 역시 부하나 로깅에 문제를 가져올 수 있다.

- 호기심이 지나친 로봇  
  로봇은 사적인 웹사이트가 public하게 노출되도록 만들 수 있다.

- 동적 게이트웨이 접근  
  로봇은 게이트웨이 애플리케이션의 콘텐츠에 대한 URL로 요청을 할 수도 있다. 이 경우로 얻은 데이터는 아마도 특수 목적을 위한 것일 테고 처리 비용이 많이 들 것이다.

<br />

## 로봇 차단하기

`robots.txt` 파일은 어떤 로봇이 서버의 어떤 부분에 접근할 수 있는지에 대한 정보가 담겨있다.

1. 콘텐츠를 요청하기 전에 `robots.txt`파일을 요청한다.
2. `robots.txt` 파일을 읽어 콘텐츠를 가져올 수 있는지 권한을 확인한다. 만약 404를 받은 경우 서버에 접근 제한이 없는 것으로 판단한다.
3. 접근이 허용된 경우 콘텐츠를 요청한다.

### 웹 사이트와 robots.txt 파일들

웹 사이트에 robots.txt 파일이 존재한다면 로봇은 반드시 그 파일을 가져와서 처리해야 한다. 그 사이트 전체에 대한 robots.txt 파일은 단 하나만이 존재한다. (만약 가상 호스팅된 사이트라면 2개 이상 존재할 수도 있다.)  
robots.txt 파일 요청 후 응답코드에 따라 로봇은 아래와 같이 대응한다.

- 200과 404인 경우 - 접근 가능
- 401 또느 403(Forbidden) - 접근 불가
- 503(Service Unavailable) - 나중에 다시 요청
- 3xx - 리다이렉션 링크 요청

### robots.txt 파일 포맷

```
User-Agent: slurp
User-Agent: webcrawler
Disallow: /private

User-Agent: *
Disallow:
```

robots.txt는 레코드별로 특정 로봇들의 집합에 대한 차단 규칙을 만들 수 있다.

- User-Agent
  - 레코드에 영향을 받는 집합을 나열한다.
  - 로봇 이름은 대소문자 구분없이 부분문자열로 검색된다.  
    로봇이 요청한 헤더에 `User-Agent: bot`이 있고, robots.txt 파일에 `User-Agent: ROBOT`이 있다면 해당 요청은 허용된다.
- Allow와 Disallow
  - User-Agent가 접근할 수 있는 URL과 접근할 수 없는 URL이 기술된다.
  - 만약 요청 링크가 Allow와 Disallow 모두 포함되지 않는다면 로봇은 해당 링크에 접근할 수 있다.
  - 대소문자를 구분하는 접두어로 매칭된다.  
     |규칙|요청 URL 경로| 접근 가능 여부
    |--|--|--|
    |/tmp|/tmpfile.html| O |
    |/tmp|/tmp/a.html|O|
    |/tmp|/tmp/|X|
    ||README.md|O(빈문자열)|
    |/~fred/hi.html|%7Efred/hi.html|O(이스케이핑 허용, 반대의 경우도 허용)
    |/~fred/hi.html|~fred%2Fhi.html|X(빗금은 이스케이핑 비허용)

### robots.txt의 캐싱과 만료

로봇은 주기적으로 robots.txt를 가져와서 그 결과를 캐시해야 한다. 로봇은 HTTP 응답의 Cache-Control과 Expires 헤더에 주의를 기울여야 한다.

### HTML 로봇 제어 META 태그

robots.txt 파일의 단점 중 하나는 그 파일을 콘텐츠의 작성자 개개인이 아니라 웹 사이트 관리자가 소유한다는 것이다.  
이에 대한 해결책으로 HTML 문서에 직접 로봇 제어 태그를 추가해 개별 페이지에 접근하는 것을 제한할 수 있다. 로봇 제어 HTML 태그에 따르는 로봇들은 문서를 가져올 수는 있지만, 문서를 무시할 것이다.

- 로봇 META 지시자
  ```html
  <!-- 예시 -->
  <meta name="R0B0TS" content="NOINDEX" />
  ```
  - NOINDEX  
     이 페이지를 인덱싱하거나 DB에 쌓는 것을 제한한다.
  - NOFOLLOW  
     이 페이지가 링크한 페이지를 크롤링하는 것을 제한한다.
  - INDEX  
     인덱싱하는 것을 허용한다.
  - FOLLOW  
     이 페이지가 링크한 페이지를 크롤링하는 것을 허용한다.
  - NOARCHIVE  
     캐시하는 것을 제한한다.
  - ALL  
     인덱싱과 크롤링을 허용한다.
  - NONE  
     인덱싱과 크롤링을 제한한다.
- 검색엔진 META 태그  
   검색엔진 META 태그는 콘텐츠를 인덱싱하는 검색엔진 로봇에 유용하다.
  ```html
  <!-- 예시 -->
  <meta name="description" content=""〉
  ```
  - description  
     웹페이지의 짧은 요약을 정의한다.
  - keywords  
     키워드 검색을 돕기 위한, 웹페이지를 기술하는 단어들의 목록을 나타낸다.
  - revisit-after  
     콘텐츠가 자주 변경되기 때문에 지정된 만큼의 날짜가 지난 이후에 다시 방문해야 한다고 지시한다.

<br />

## 검색엔진

오늘날 웹에는 수많은 웹 페이지가 있기 때문에 웹 전체를 크롤링하는 것은 옳지 않은 방법이다. 대신 검색엔진은 전 세계의 웹페이지들에 대해 `풀 텍스트 인덱스(full-text indexes)`라는 복잡한 로컬 데이터베이스를 생성한다.

### 현대적인 검색엔진의 아키텍처

검색엔진 크롤러들은 웹페이지들을 수집하여 풀 텍스트 인덱스에 추가한다. 풀 텍스트 인덱스는 웹의 특정 순간에 대한 스냅숏에 불과하다.

### 풀 텍스트 색인

풀 텍스트 인덱스는 단어 하나를 입력받아 그 단어를 포함하고 있는 문서를 즉각 알려줄 수 있는 데이터베이스다. 이 문서들은 색인이 생성된 후에는 검색할 필요가 없다.

<img width="367" alt="image" src="https://github.com/flataex/http-the-definitive-guide-study/assets/67260437/3d9e4930-fa1d-4170-8daf-e432831d7155">

### 질의 보내기

사용자는 웹 검색엔진 게이트웨이로 검색 질의를 전송한다. 게이트웨이 프로그램은 이 질의를 받아 풀 텍스트 색인을 검색할 때 사용되는 표현식으로 변환한다. 그 후 게이트웨이는 웹 서버에게 문서의 목록을 결과로 돌려주고, 웹 서버는 이 결과를 사용자를 위한 HTML 페이지로 변환한다.
